{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c84e59-b6b9-4353-a2f3-e23095e3d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Forward propagation is where input data is fed through a network, in a forward direction, to generate an output. The data is accepted by hidden layers and processed, as per the activation function, and moves to the successive layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86e7c55-df44-4671-80ed-22d56ecf1da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2During forward propagation at each node of hidden and output layer preactivation and activation takes place. For example at the first node of the hidden layer, a1(preactivation) is calculated first and then h1(activation) is calculated. a1 is a weighted sum of inputs. Here, the weights are randomly generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd4db2a-e0a6-4bde-b32f-17e112877e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3The input is used to calculate some intermediate function in the hidden layer, which is then used to calculate an output. In the feedforward propagation, the Activation Function is a mathematical “gate” in between the input feeding the current neuron and its output going to the next laye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6d416f-d301-4e5d-8e38-72bcd0c2278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4Weights and biases are neural network parameters that simplify machine learning data identification. The weights and biases develop how a neural network propels data flow forward through the network; this is called forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac1c5f67-7246-4d33-b32d-6b107e45d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5Simply put, the softmax activation function forces the values of the output neurons to take values between zero and one, so that they can represent probability values in the interval [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebcc62d3-1131-4263-b113-3c239b2e298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6Backpropagation is the process of adjusting the weights of a neural network by analyzing the error rate from the previous iteration. Hinted at by its name, backpropagation involves working backward from outputs to inputs to figure out how to reduce the number of errors and make a neural network more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9219287-62b9-4d21-a696-f41075cd96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7Matrix Form of the Backward Propagation\n",
    "#Wih is the weight matrix between the input and the hidden layer with the dimension of 4*5. WihT, is the transpose of Wih, having shape 5*4. X is the input variables having dimension 4*5, and. bih is a bias term, has a single value here as considering the same for all the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "061c6821-cc64-47b3-a6b2-e35bb716f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8The chain rule can be generalised to multivariate functions, and represented by a tree diagram. The chain rule is applied extensively by the backpropagation algorithm in order to calculate the error gradient of the loss function with respect to each weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7925a-9876-4663-9282-92514fc40d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
